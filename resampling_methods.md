Resampling Methods
================
Lin Yang

``` r
library(FNN)
library(caret)
```

# Generate a simulated training dataset

``` r
gen_data <- function(n) {
  x1 <- rnorm(n, mean = 1)
  x2 <- rnorm(n, mean = 1)
  eps <- rnorm(n, sd = 0.5)
  y <- sin(x1) + (x2)^2 + eps
  data.frame(Y = y, X1 = x1, X2 = x2)
}

set.seed(2022)
trainData <- gen_data(200)
trainData
```

    ##               Y           X1           X2
    ## 1    3.25360688  1.900141986  1.376474562
    ## 2   -0.59907000 -0.173345773  0.805460391
    ## 3    1.48418312  0.102514639 -1.115336023
    ## 4    0.11115225 -0.444501400  0.086932598
    ## 5    3.30184248  0.668986422  1.349709252
    ## 6    1.17953284 -1.900628987  1.405577108
    ## 7    2.75649110 -0.059255728  1.505326124
    ## 8    1.84178995  1.277954738 -0.791867106
    ## 9    1.34339283  1.749485918  0.338511065
    ## 10   0.83596096  1.241582537  0.627727155
    ## 11   2.54613094  2.006185703  1.158661009
    ## 12   0.71929058  0.814853975  0.321587584
    ## 13   0.55755558  0.018173287  1.082489028
    ## 14   5.82822558  1.092907945  2.243740109
    ## 15   4.45405211  0.947215604  1.880495548
    ## 16   2.72019005  0.919672101  1.543958728
    ## 17  -0.08704025  0.345896333  0.209094873
    ## 18   1.92175602  0.049316491  1.427144697
    ## 19   5.37892397  2.019561763  1.951114317
    ## 20   4.26131273  1.859046412  1.820117090
    ## 21   1.49664318  1.364460784 -0.810945971
    ## 22   1.16903653  1.383651001  0.720086363
    ## 23   4.67778036  2.113405717  1.671068256
    ## 24   7.03643152  2.211509787  2.596972531
    ## 25   1.27925174  0.651674538  0.274268171
    ## 26   4.13294514  0.140446553  1.960575166
    ## 27   4.90203352  1.650027188  1.956439585
    ## 28   6.23427155  1.328059131  2.265847687
    ## 29   1.31786358  0.482053427  0.763043827
    ## 30   0.43131868  0.761017851 -0.520114076
    ## 31   3.16346949  1.117778855  1.339553425
    ## 32   6.56518991  1.831518453  2.395973911
    ## 33  -0.90915406 -0.558918919 -0.186062223
    ## 34   2.03880902  0.779480931  0.595528450
    ## 35   0.24210720  0.182805570  0.712893850
    ## 36   5.87278990  2.076677442  1.942585748
    ## 37   0.60960086  2.079664004 -0.054738590
    ## 38   1.07146905  1.142128105  0.342722490
    ## 39   0.54657997  1.156979546  0.585711130
    ## 40   2.74449547  0.831279735  1.453966694
    ## 41   0.71953415  0.730962308  0.446046027
    ## 42   2.60295021  1.807768419  1.353868528
    ## 43   0.95104572 -0.124717239  1.148285817
    ## 44   0.77644397 -0.430788022  1.376511915
    ## 45   7.33794238  1.060356678  2.506987104
    ## 46   1.15743478  0.207017505  0.072705048
    ## 47   1.12529945  1.340275932  0.412858769
    ## 48  12.21357283  0.740531270  3.458925710
    ## 49   2.73777461 -0.304848647  1.673741881
    ## 50   2.84270329  1.368173442  1.334424367
    ## 51   6.55335523  2.693190016  2.609041937
    ## 52   1.98895277  1.995837022 -0.115399258
    ## 53   1.71983438  1.186752135  1.010130070
    ## 54   0.87297539  2.238337398 -0.052357937
    ## 55   2.05867167  1.309373297  1.433923240
    ## 56   4.41942736  1.635717929  1.658089617
    ## 57   0.79643405  1.023183275 -0.440073331
    ## 58   3.03861970  2.177863603  1.473361209
    ## 59  -1.18026246  0.546453400  0.194147882
    ## 60   0.52447227  1.415927539  0.214534074
    ## 61  -0.98247485 -0.338442391 -0.135350602
    ## 62  -0.43003519 -0.291974738  0.088574823
    ## 63   2.10700777  0.690925786  1.089966273
    ## 64  -0.03854514  1.156512093  0.763905256
    ## 65   6.83873150  0.166083248  2.627579123
    ## 66   4.18181091  0.975450717  1.505200628
    ## 67   1.12378177 -0.137351552  1.037437302
    ## 68   3.25300378  2.072054248  1.365165210
    ## 69   0.30951055  3.314498582 -0.394600802
    ## 70   3.92892289  1.422973105  1.635134655
    ## 71   1.34030801  0.863060953  0.522247302
    ## 72   0.65978990  2.328396523  0.180025746
    ## 73   6.02058571  1.436535665  2.252384155
    ## 74   1.29318934  1.066428514 -0.318349946
    ## 75   3.13883730  2.304659810  1.544261181
    ## 76   1.10046803  0.790740461  0.813103002
    ## 77   3.15535328  2.018253037  1.615963600
    ## 78   2.03068010  2.366034963  1.527987584
    ## 79   2.11547640  2.476695849  1.234455290
    ## 80   1.53557494  1.887306531  0.887701917
    ## 81   0.26599337 -0.015908900 -0.316296913
    ## 82   2.58990157  2.870869061  0.663232478
    ## 83   3.05278908  2.076198420  1.478410170
    ## 84   0.58222473 -0.074407629 -0.093911571
    ## 85   3.87793827 -1.195575974  2.261405589
    ## 86   2.02246983  1.534544569  1.036015183
    ## 87   8.36592916  2.343733429  2.881067208
    ## 88   1.73739508  2.385003541  0.133544955
    ## 89  -1.00128447  3.746926775  0.005152236
    ## 90   0.82522472  0.954055312 -0.517233551
    ## 91   2.32056527  1.743085339  0.959822852
    ## 92   1.37009785  1.260424695 -0.786537784
    ## 93   4.58677582  1.428281904  1.956246208
    ## 94   1.87765061  0.631741246  0.719504158
    ## 95  -0.19980191  3.887423304  0.588812672
    ## 96   0.50249356  0.392927356  0.536569555
    ## 97   1.06950434 -0.879198737 -1.454937009
    ## 98   3.16761979  1.718356569  1.312886330
    ## 99   0.57840180  1.251425569  0.208661823
    ## 100  3.18889752  1.467022973  1.225465009
    ## 101  1.29271289  0.782111620 -0.783807844
    ## 102  0.17712332 -0.217647707 -0.369828833
    ## 103  1.65979225  0.245921495  1.393976694
    ## 104  2.27915471  1.191230181  1.267222048
    ## 105  2.66638651  2.047282765  1.087235709
    ## 106  6.72569935  2.089828385  2.551864935
    ## 107  4.05966883  1.695474419  1.795528695
    ## 108  2.12942321  0.475631244  1.341340123
    ## 109  1.76445785  1.264015929 -0.109809817
    ## 110  1.06392454  2.241200393 -0.042362128
    ## 111  1.13539402  1.643297239  0.811005160
    ## 112  4.99683289  0.003237391  2.286030721
    ## 113 -0.40992296 -1.002298454 -0.431189489
    ## 114  0.84782807  0.524160893  0.612539470
    ## 115  1.53703489  1.044179982  0.497397062
    ## 116 -0.41216448 -0.381135358  1.067550486
    ## 117  3.47467740  1.247519655  1.506546455
    ## 118  0.75788824  1.224189650 -0.281615688
    ## 119  0.94048080  1.861478944 -0.055354030
    ## 120  0.28172574  0.073237735  0.517821217
    ## 121  3.11930838  0.679556995  1.728531884
    ## 122  1.72664767  1.673024777  0.781619565
    ## 123  4.97344763 -0.265629757  2.308913137
    ## 124  1.85578913  0.481479037  0.749298401
    ## 125  1.01969611  2.122494126  0.454345083
    ## 126  5.26495128  0.684022464  2.230907352
    ## 127  0.05718880 -0.758299204  0.462171201
    ## 128  3.26390400  2.237253438  1.595821783
    ## 129  1.03254870 -0.663952369  1.451962972
    ## 130  0.34501659  2.855734456  0.335795799
    ## 131  3.18450386  2.057997415 -1.227765454
    ## 132  1.62122549  1.037268817  0.999271096
    ## 133  6.99064892  1.004605023  2.358858675
    ## 134 -0.77455748 -0.943834273 -0.327559237
    ## 135  0.66359928 -0.569623410  1.249763076
    ## 136  0.82968136  0.726702841 -0.160135967
    ## 137  5.95580343  1.468853820  2.254586067
    ## 138  1.23775038  2.032538762  0.722653437
    ## 139  0.80531874 -0.418396104  0.772819024
    ## 140  0.70071723  0.912122497  0.545108103
    ## 141  1.55689481  2.578630748  0.887297380
    ## 142  1.99255103  1.244624932  0.965865678
    ## 143  0.15180309  2.497093497  0.601909066
    ## 144  1.21140685  1.915906419  0.665690601
    ## 145  3.66877179  1.320841191  1.559180638
    ## 146  1.27348771  1.562724275  0.757044398
    ## 147  1.63004256  1.539295318 -0.690085139
    ## 148  0.09351900 -0.073404911 -0.295097695
    ## 149  3.94071516 -0.883160628  2.030397306
    ## 150  0.42458582  1.911390201  0.226315655
    ## 151  3.60806694  1.049596352  1.397466473
    ## 152 -0.95979409 -0.235654906  0.267917839
    ## 153  2.63508422 -0.263751908  1.556583821
    ## 154  1.12619169 -0.142864958  0.473312064
    ## 155  1.32343486  1.654411752 -0.010173135
    ## 156  1.15128940  0.323180553  0.696249608
    ## 157 -0.53648941 -0.073910491  0.007244767
    ## 158  2.56260548  0.024341871  1.573000720
    ## 159  2.12216161 -0.126274625  1.562025322
    ## 160 -0.89275222 -0.969312730  0.417394016
    ## 161  3.82517775  0.418011023  2.020378781
    ## 162 -0.45429418 -0.762507139  0.677757302
    ## 163  1.18404008  0.472173993  0.960912429
    ## 164  0.72261194 -0.374327581  1.231006486
    ## 165  4.58947898 -0.933712605  2.348438191
    ## 166  6.97047958  1.979783093  2.326104756
    ## 167  0.92011145 -0.495689509  0.951902221
    ## 168  0.36649380  2.700809549  0.402786451
    ## 169  2.17733788 -0.401025690  1.607736809
    ## 170  3.46804719  0.515536955  1.820151625
    ## 171  0.61424296 -0.216178616  0.792646617
    ## 172  0.50560043  1.104770519 -0.286288582
    ## 173  2.33055885  0.757462890  1.405406502
    ## 174  2.20729665 -0.415781046  1.575597165
    ## 175  0.29674313 -0.049459838  0.340195744
    ## 176  0.62716035  1.269056970 -0.045509580
    ## 177  2.92319150  0.521283839  1.333458831
    ## 178  0.42166680  0.700551632  0.269385898
    ## 179  1.06718970  2.038148508  0.904057971
    ## 180  8.62675510  2.028925116  2.679318448
    ## 181 -0.55213055 -0.971675197  0.713804241
    ## 182  3.86079557 -0.283329902  1.964314960
    ## 183  0.61683200  1.048581252  0.278438415
    ## 184  1.96461811  2.210631089  1.231152766
    ## 185  7.94126604  0.515329366  2.812150072
    ## 186  0.55280217  0.206690388  0.390155105
    ## 187  1.60366495  1.172414357  0.055935688
    ## 188  3.12365025  2.105213104  1.547383687
    ## 189  3.42721251  1.740875529  1.327824356
    ## 190  1.13006633  0.758553351  0.606458112
    ## 191  1.53935100  1.614957442 -0.015197154
    ## 192  1.23965847  2.382546469  0.373373037
    ## 193  4.23023881  0.416797440  1.782335058
    ## 194 14.96175352  0.802069924  3.693778528
    ## 195  8.22564708  0.396770521  2.864219261
    ## 196  4.55738068 -0.088648401  2.178384540
    ## 197  1.48384707  1.183915274  0.827205992
    ## 198  1.51397161  2.307136295 -0.976334585
    ## 199  0.90496467  0.831716709  0.758985230
    ## 200  1.61013153  1.343840748 -0.044990739

# Data splitting functions

## Training/Validation Splitting

``` r
vsSplits <- createDataPartition(y = trainData$Y,
                                times = 2,
                                p = 0.8,
                                groups = 5,
                                list = FALSE)
vsSplits
```

    ##        Resample1 Resample2
    ##   [1,]         1         2
    ##   [2,]         2         3
    ##   [3,]         4         4
    ##   [4,]         5         5
    ##   [5,]         6         6
    ##   [6,]         8         7
    ##   [7,]         9         8
    ##   [8,]        10         9
    ##   [9,]        12        12
    ##  [10,]        14        13
    ##  [11,]        15        15
    ##  [12,]        16        16
    ##  [13,]        17        17
    ##  [14,]        18        18
    ##  [15,]        19        19
    ##  [16,]        21        20
    ##  [17,]        22        21
    ##  [18,]        24        22
    ##  [19,]        25        23
    ##  [20,]        26        24
    ##  [21,]        27        25
    ##  [22,]        29        27
    ##  [23,]        30        28
    ##  [24,]        31        30
    ##  [25,]        32        31
    ##  [26,]        34        32
    ##  [27,]        35        34
    ##  [28,]        36        35
    ##  [29,]        38        36
    ##  [30,]        39        37
    ##  [31,]        41        38
    ##  [32,]        43        39
    ##  [33,]        44        40
    ##  [34,]        46        41
    ##  [35,]        47        42
    ##  [36,]        48        43
    ##  [37,]        49        44
    ##  [38,]        50        45
    ##  [39,]        51        46
    ##  [40,]        52        47
    ##  [41,]        53        48
    ##  [42,]        54        49
    ##  [43,]        55        50
    ##  [44,]        56        51
    ##  [45,]        57        52
    ##  [46,]        58        53
    ##  [47,]        59        54
    ##  [48,]        60        55
    ##  [49,]        61        58
    ##  [50,]        62        59
    ##  [51,]        63        62
    ##  [52,]        64        63
    ##  [53,]        65        65
    ##  [54,]        66        66
    ##  [55,]        67        67
    ##  [56,]        69        70
    ##  [57,]        70        71
    ##  [58,]        71        73
    ##  [59,]        72        74
    ##  [60,]        74        75
    ##  [61,]        77        78
    ##  [62,]        78        79
    ##  [63,]        79        80
    ##  [64,]        80        82
    ##  [65,]        81        84
    ##  [66,]        82        85
    ##  [67,]        83        86
    ##  [68,]        84        87
    ##  [69,]        85        89
    ##  [70,]        87        90
    ##  [71,]        89        91
    ##  [72,]        91        93
    ##  [73,]        92        94
    ##  [74,]        94        95
    ##  [75,]        95        96
    ##  [76,]        96        97
    ##  [77,]        97        98
    ##  [78,]        98        99
    ##  [79,]        99       100
    ##  [80,]       100       102
    ##  [81,]       101       103
    ##  [82,]       102       104
    ##  [83,]       103       105
    ##  [84,]       104       106
    ##  [85,]       105       107
    ##  [86,]       106       109
    ##  [87,]       108       110
    ##  [88,]       109       111
    ##  [89,]       111       112
    ##  [90,]       112       113
    ##  [91,]       113       115
    ##  [92,]       116       116
    ##  [93,]       117       117
    ##  [94,]       119       119
    ##  [95,]       121       120
    ##  [96,]       122       125
    ##  [97,]       123       126
    ##  [98,]       124       127
    ##  [99,]       125       128
    ## [100,]       126       129
    ## [101,]       127       130
    ## [102,]       128       131
    ## [103,]       129       132
    ## [104,]       130       133
    ## [105,]       131       134
    ## [106,]       132       135
    ## [107,]       133       136
    ## [108,]       136       137
    ## [109,]       137       138
    ## [110,]       138       139
    ## [111,]       139       140
    ## [112,]       141       141
    ## [113,]       143       142
    ## [114,]       144       143
    ## [115,]       145       144
    ## [116,]       146       147
    ## [117,]       147       148
    ## [118,]       148       149
    ## [119,]       150       150
    ## [120,]       151       151
    ## [121,]       153       152
    ## [122,]       154       153
    ## [123,]       155       154
    ## [124,]       156       155
    ## [125,]       157       156
    ## [126,]       158       158
    ## [127,]       159       160
    ## [128,]       161       161
    ## [129,]       162       162
    ## [130,]       164       163
    ## [131,]       165       165
    ## [132,]       166       167
    ## [133,]       167       168
    ## [134,]       168       169
    ## [135,]       169       171
    ## [136,]       170       172
    ## [137,]       171       174
    ## [138,]       172       175
    ## [139,]       173       176
    ## [140,]       174       177
    ## [141,]       175       178
    ## [142,]       178       181
    ## [143,]       180       182
    ## [144,]       181       184
    ## [145,]       182       185
    ## [146,]       183       186
    ## [147,]       184       187
    ## [148,]       185       188
    ## [149,]       186       189
    ## [150,]       187       190
    ## [151,]       189       191
    ## [152,]       190       192
    ## [153,]       191       193
    ## [154,]       192       194
    ## [155,]       194       195
    ## [156,]       195       196
    ## [157,]       196       197
    ## [158,]       197       198
    ## [159,]       198       199
    ## [160,]       200       200

## K-fold CV

``` r
#10-fold CV
set.seed(1)
cvSplits <- createFolds(y = trainData$Y,
                        k = 10,
                        returnTrain = TRUE)
str(cvSplits)
```

    ## List of 10
    ##  $ Fold01: int [1:180] 1 2 3 4 5 6 8 10 11 12 ...
    ##  $ Fold02: int [1:180] 2 3 4 5 6 7 8 9 10 11 ...
    ##  $ Fold03: int [1:180] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ Fold04: int [1:180] 1 2 4 5 6 7 8 9 11 12 ...
    ##  $ Fold05: int [1:180] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ Fold06: int [1:180] 1 3 4 5 6 7 8 9 10 12 ...
    ##  $ Fold07: int [1:180] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ Fold08: int [1:180] 1 2 3 4 5 6 7 8 9 10 ...
    ##  $ Fold09: int [1:180] 1 2 3 7 8 9 10 11 12 13 ...
    ##  $ Fold10: int [1:180] 1 2 3 4 5 6 7 9 10 11 ...

``` r
#repeated 10-fold CV
set.seed(1)
rcvSplits <- createMultiFolds(y = trainData$Y,
                              k= 10,
                              times = 5)
length(rcvSplits)
```

    ## [1] 50

A simple example:

``` r
K <- length(rcvSplits)
mseK_lm <- rep(NA, K)
mseK_knn <- rep(NA, K)

for (k in 1:K) {
  trRows <- rcvSplits[[k]]
  
  fit_lm <- lm(Y ~ X1 + X2, data = trainData[trRows,])
  pred_lm <- predict(fit_lm, trainData[-trRows,])
  pred_knn <- knn.reg(train = trainData[trRows, 2:3],
                      test = trainData[-trRows, 2:3],
                      y = trainData$Y[trRows], k = 3)
  mseK_lm[k] <- mean((trainData$Y[-trRows] - pred_lm)^2)
  mseK_knn[k] <- mean((trainData$Y[-trRows] - pred_knn$pred)^2)
}

c(mean(mseK_lm), mean(mseK_knn))
```

    ## [1] 1.9092396 0.6483281

# Specify the resampling method using `trainControl`

``` r
# K-fold CV
ctrl1 <- trainControl(method = 'cv', number = 10)
# leave-one-out CV
ctrl2 <- trainControl(method = 'LOOCV')
# leave-group-out / Monte Carlo CV
ctrl3 <- trainControl(method = 'LGOCV', p = 0.75, number = 50)
# 632 bootstrap
ctrl4 <- trainControl(method = 'boot632', number = 100)
# repeated K-fold CV
ctrl5 <- trainControl(method = 'repeatedcv', repeats = 5, number = 10)
# user-specified folds
ctrl7 <- trainControl(index = rcvSplits)
# only fit one model to the entire training set
ctrl6 <- trainControl(method = 'none')
```

``` r
set.seed(1)
lmFit <- train(Y~.,
               data = trainData,
               method = 'lm',
               trControl = ctrl4)

set.seed(1)
knnFit <- train(Y~.,
                data = trainData,
                method = 'knn',
                trControl = ctrl4)

identical(lmFit$control$index, knnFit$control$index)
```

    ## [1] TRUE

``` r
lmFit2 <- train(Y~.,
                data = trainData,
                method = 'lm',
                trControl = ctrl7)
mean((lmFit2$resample$RMSE)^2)
```

    ## [1] 1.90924

``` r
mean(mseK_lm)
```

    ## [1] 1.90924

``` r
knnFit2 <- train(Y~.,
                 data = trainData,
                 method = 'knn',
                 tuneGrid = data.frame(k = 3),
                 trControl = ctrl7)
mean((knnFit2$resample$RMSE)^2)
```

    ## [1] 0.6483281

``` r
mean(mseK_knn)
```

    ## [1] 0.6483281

To compare these two models based on their cross-validation statistics,
the `resamples()` function can be used with models that share a *common*
set of resampled datasets.

``` r
resamp <- resamples(list(lm = lmFit, knn = knnFit))
summary(resamp)
```

    ## 
    ## Call:
    ## summary.resamples(object = resamp)
    ## 
    ## Models: lm, knn 
    ## Number of resamples: 100 
    ## 
    ## MAE 
    ##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
    ## lm  0.7945741 0.9186118 0.9725452 0.9765637 1.0374921 1.2930356    0
    ## knn 0.4894948 0.5780002 0.6134864 0.6188583 0.6713049 0.7955981    0
    ## 
    ## RMSE 
    ##          Min.   1st Qu.    Median      Mean   3rd Qu.     Max. NA's
    ## lm  1.0226799 1.2323791 1.3469670 1.3608658 1.4952322 1.870128    0
    ## knn 0.5976467 0.7256573 0.7786223 0.8720233 0.9974258 1.525634    0
    ## 
    ## Rsquared 
    ##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
    ## lm  0.5196270 0.6403360 0.6877803 0.6805366 0.7195194 0.7887630    0
    ## knn 0.7752867 0.8444285 0.8699985 0.8683677 0.8953846 0.9204918    0
